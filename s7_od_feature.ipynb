{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils.data_loader as data_loader\n",
    "import utils.display as display\n",
    "import utils.vector_haversine_distances as vec_hs_dis\n",
    "from scipy import stats\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "import sklearn.metrics\n",
    "import sklearn\n",
    "from hotspot.hotpots_discovery_utils import generate_cube_index, cube_to_coordinate\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.configure_logging()\n",
    "display.configure_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "def mape_vectorized_v2(a, b):\n",
    "    mask = a != 0\n",
    "    return (np.fabs(a - b)/a)[mask].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20-Jul-20 23:01:13 - Loading data/transaction_201407.csv\n"
     ]
    }
   ],
   "source": [
    "df_od = data_loader.load_od(scale='full', common=False)\n",
    "# 筛选出处于bbox中的points\n",
    "df_od['in_bbox'] = ((113.764635 < df_od['destination_log'])\n",
    "                     & (df_od['destination_log'] < 114.608972)\n",
    "                     & (22.454727 < df_od['destination_lat'])\n",
    "                     & (df_od['destination_lat'] < 22.842654)\n",
    "                     & (113.764635 < df_od['original_log'])\n",
    "                     & (df_od['original_log'] < 114.608972)\n",
    "                     & (22.454727 < df_od['original_lat'])\n",
    "                     & (df_od['original_lat'] < 22.842654))\n",
    "df_od = df_od.loc[df_od.in_bbox].reset_index(drop=True)\n",
    "df_od = generate_cube_index(df_od, m=100, n=200)\n",
    "\n",
    "demand = df_od.groupby(['original_cube', 'destination_cube']).size().reset_index()\n",
    "demand = demand.rename(columns={0: 'demand'})\n",
    "\n",
    "demand = demand.loc[demand['demand'] > 10].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20-Jul-20 23:01:46 - Loading data/transaction_common_201407.csv\n"
     ]
    }
   ],
   "source": [
    "df_et_od = data_loader.load_od(scale='full', common=True)\n",
    "# 筛选出处于bbox中的points\n",
    "df_et_od['in_bbox'] = ((113.764635 < df_et_od['destination_log'])\n",
    "                     & (df_et_od['destination_log'] < 114.608972)\n",
    "                     & (22.454727 < df_et_od['destination_lat'])\n",
    "                     & (df_et_od['destination_lat'] < 22.842654)\n",
    "                     & (113.764635 < df_et_od['original_log'])\n",
    "                     & (df_et_od['original_log'] < 114.608972)\n",
    "                     & (22.454727 < df_et_od['original_lat'])\n",
    "                     & (df_et_od['original_lat'] < 22.842654))\n",
    "df_et_od = df_et_od.loc[df_et_od.in_bbox].reset_index(drop=True)\n",
    "df_et_od = generate_cube_index(df_et_od, m=100, n=200)\n",
    "\n",
    "et_demand = df_et_od.groupby(['original_cube', 'destination_cube']).size().reset_index()\n",
    "et_demand = et_demand.rename(columns={0: 'demand'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demands = pd.merge(demand, et_demand, how='left', on=['original_cube', 'destination_cube'], suffixes=('_all', '_et'))\n",
    "df_demands = df_demands.fillna(0)\n",
    "df_demands['rate'] = df_demands['demand_et'] / df_demands['demand_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "df_od['duration'] = (df_od['end_time'] - df_od['begin_time']).dt.total_seconds()\n",
    "df_od = df_od[['original_cube', 'destination_cube', 'original_log', 'original_lat', 'destination_log',\n",
    "               'destination_lat', 'duration']].groupby(['original_cube', 'destination_cube']).mean()\n",
    "df_od.reset_index(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20-Jul-20 23:01:49 - Loading 'C:\\Users\\hkrep\\PycharmProjects\\ChargingEventsExtraction\\data\\ChargingStation'\n"
     ]
    }
   ],
   "source": [
    "df_od_pairs = pd.merge(df_demands[['original_cube', 'destination_cube']],\n",
    "                       df_od[['original_cube', 'destination_cube', 'original_log', 'original_lat', 'destination_log',\n",
    "                              'destination_lat', 'duration']],\n",
    "                       left_on=['original_cube', 'destination_cube'],\n",
    "                       right_on=['original_cube', 'destination_cube'])\n",
    "\n",
    "# df_od_pairs = df_od_pairs.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_cs, date = data_loader.load_cs(scale='part', date=datetime.datetime(2014, 7, 1))\n",
    "df_cs = df_cs.loc[~df_cs['cs_name'].isin(['LJDL', 'E04', 'BN0002', 'F11', 'S1', 'S2', 'F12', 'F13', 'F15'])]\n",
    "df_cs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# select drop location and CS location as two array\n",
    "original_locations = df_od_pairs[['original_lat', 'original_log']].to_numpy()\n",
    "destination_locations = df_od_pairs[['destination_lat', 'destination_log']].to_numpy()\n",
    "cs_location = df_cs[['Latitude', 'Longitude']].to_numpy()\n",
    "\n",
    "# earth radius(km)\n",
    "AVG_EARTH_RADIUS = 6371.0088\n",
    "\n",
    "# calculate distance between original/destination location and CS location\n",
    "original_distances_to_cs = haversine_distances(np.radians(original_locations), np.radians(cs_location)) \\\n",
    "                           * AVG_EARTH_RADIUS\n",
    "destination_distances_to_cs = haversine_distances(np.radians(destination_locations), np.radians(cs_location)) \\\n",
    "                              * AVG_EARTH_RADIUS\n",
    "# calculate distance between od locations\n",
    "od_dis = vec_hs_dis.haversine_np(df_od_pairs['original_log'], df_od_pairs['original_lat'],\n",
    "                                 df_od_pairs['destination_log'], df_od_pairs['destination_lat'])\n",
    "\n",
    "df_original_distances_to_cs = pd.DataFrame(original_distances_to_cs)\n",
    "df_destination_distances_to_cs = pd.DataFrame(destination_distances_to_cs)\n",
    "df_od_dis = pd.DataFrame(od_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186036 sample\n",
      "2 nearest cs as feature:\n",
      "(148828, 10) (148828,)\n",
      "(37208, 10) (37208,)\n"
     ]
    }
   ],
   "source": [
    "def evaluate(n=10):\n",
    "    original_capacity = np.repeat(df_cs['chg_points'].values.reshape(1, -1), df_od_pairs.shape[0], axis=0)\n",
    "    \n",
    "    a = df_original_distances_to_cs.values\n",
    "    a.sort(axis=1)\n",
    "    a = pd.DataFrame(a, df_original_distances_to_cs.index, df_original_distances_to_cs.columns)\n",
    "    a = a.iloc[:, :n]\n",
    "    o_dissorted_capacity = np.take_along_axis(original_capacity, df_original_distances_to_cs.values.argsort(axis=1),\n",
    "                                              axis=1)\n",
    "    o_dissorted_capacity = pd.DataFrame(o_dissorted_capacity)\n",
    "    o_dissorted_capacity = o_dissorted_capacity.iloc[:, :n]\n",
    "    \n",
    "    b = df_destination_distances_to_cs.values\n",
    "    b.sort(axis=1)\n",
    "    b = pd.DataFrame(b, df_destination_distances_to_cs.index, df_destination_distances_to_cs.columns)\n",
    "    b = b.iloc[:, :n]\n",
    "    d_dissorted_capacity = np.take_along_axis(original_capacity, df_destination_distances_to_cs.values.argsort(axis=1),\n",
    "                                              axis=1)\n",
    "    d_dissorted_capacity = pd.DataFrame(d_dissorted_capacity)\n",
    "    d_dissorted_capacity = d_dissorted_capacity.iloc[:, :n]\n",
    "    \n",
    "    train_x = pd.concat([df_od_pairs['duration'], od_dis, a, o_dissorted_capacity, b, d_dissorted_capacity],\n",
    "                        axis=1).iloc[:int(0.7*a.shape[0])]\n",
    "    test_x = pd.concat([df_od_pairs['duration'], od_dis, a, o_dissorted_capacity, b, d_dissorted_capacity],\n",
    "                       axis=1).iloc[int(0.7*a.shape[0]):]\n",
    "    train_y = df_demands['rate'].iloc[:int(0.7*a.shape[0])]\n",
    "    test_y = df_demands['rate'].iloc[int(0.7*a.shape[0]):]\n",
    "#     print(train_x.iloc[:3], train_y.iloc[:3])\n",
    "#     print(test_x.iloc[:3], test_y.iloc[:3])\n",
    "    gbm = xgboost.XGBRegressor(verbosity=0, n_estimators=100, scale_pos_weight=2,\n",
    "                               validate_parameters=2, learning_rate=0.05,\n",
    "                               min_child_weight=7) #这行会有个提示，不用管\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth': [4, 8, 10],\n",
    "        # 'gamma': [0, 5,],\n",
    "        # 'max_delta_step': [0, 3],\n",
    "        # 'subsample': [0.7, 1],\n",
    "        # 'colsample_bytree': [0.7, 1],\n",
    "        # 'reg_alpha': [0, 0.1],\n",
    "        # 'reg_lambda': [1, 3,]\n",
    "    }\n",
    "    # gs = GridSearchCV(gbm, param_grid=param_grid, cv=5, scoring=make_scorer(lambda x, y: 10 - mape_vectorized_v2(x, y)))\n",
    "    gs = GridSearchCV(gbm, param_grid=param_grid, cv=5, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "    train_x, val_x, train_y, val_y = train_test_split(\n",
    "        pd.concat([df_od_pairs['duration'], od_dis, a, o_dissorted_capacity, b, d_dissorted_capacity],axis=1).values,\n",
    "        df_demands['rate'].values, test_size=0.2)\n",
    "    print(train_x.shape, train_y.shape)\n",
    "    print(val_x.shape, val_y.shape)\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    train_x = scaler.fit_transform(train_x)\n",
    "    val_x = scaler.transform(val_x)\n",
    "\n",
    "    gs.fit(train_x, train_y)\n",
    "\n",
    "    print(gs.best_params_)\n",
    "    print(gs.best_score_)\n",
    "    print(gs.best_estimator_)\n",
    "    print(gs.best_index_)\n",
    "\n",
    "    gs.best_estimator_.fit(train_x, train_y)\n",
    "    predict_y = gs.best_estimator_.predict(val_x)\n",
    "\n",
    "    print('mape:', mape_vectorized_v2(val_y.reshape(-1), predict_y))\n",
    "    print('xgb score (rmse in xgb doc):', gs.best_estimator_.score(val_x, val_y))\n",
    "    print('gs score (neg rmse):', gs.score(val_x, val_y))\n",
    "    print('sample_gt:', val_y[10:15])\n",
    "    print('sample_pred:', predict_y[10:15])\n",
    "\n",
    "print(df_od_pairs.shape[0], 'sample')\n",
    "for i in range(2, 3):\n",
    "    print(i, 'nearest cs as feature:')\n",
    "    evaluate(n=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method RegressorMixin.score of XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n             colsample_bynode=None, colsample_bytree=None, gamma=None,\n             gpu_id=None, importance_type='gain', interaction_constraints=None,\n             learning_rate=0.05, max_delta_step=None, max_depth=None,\n             min_child_weight=7, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             random_state=None, reg_alpha=None, reg_lambda=None,\n             scale_pos_weight=2, subsample=None, tree_method=None,\n             validate_parameters=2, verbosity=0)>"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = xgboost.XGBRegressor(verbosity=0, n_estimators=100, scale_pos_weight=2,\n",
    "                               validate_parameters=2, learning_rate=0.05,\n",
    "                               min_child_weight=7)\n",
    "gbm.score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186036 sample\n",
      "2 nearest cs as feature:\n",
      "mae: 0.8729232793155222 mse: 1.3723586656594307\n",
      "mape: -0.02497133495736209\n",
      "n_rmse: 0.02296742591684664\n",
      "[[-0.5398047 ]\n",
      " [-0.5398047 ]\n",
      " [ 1.82452057]\n",
      " [ 1.19403383]\n",
      " [-0.5398047 ]\n",
      " [ 0.18262802]\n",
      " [-0.5398047 ]\n",
      " [ 4.41401967]\n",
      " [ 3.79479162]\n",
      " [-0.5398047 ]] [0.38607877 0.35451627 0.35635728 0.38015684 0.38015684 0.3589432\n",
      " 0.31963408 0.27654982 0.3461758  0.31509438]\n"
     ]
    }
   ],
   "source": [
    "def evaluate(n=10):\n",
    "    original_capacity = np.repeat(df_cs['chg_points'].values.reshape(1, -1), df_od_pairs.shape[0], axis=0)\n",
    "\n",
    "    a = df_original_distances_to_cs.values\n",
    "    a.sort(axis=1)\n",
    "    a = pd.DataFrame(a, df_original_distances_to_cs.index, df_original_distances_to_cs.columns)\n",
    "    a = a.iloc[:, :n]\n",
    "    o_dissorted_capacity = np.take_along_axis(original_capacity, df_original_distances_to_cs.values.argsort(axis=1),\n",
    "                                              axis=1)\n",
    "    o_dissorted_capacity = pd.DataFrame(o_dissorted_capacity)\n",
    "    o_dissorted_capacity = o_dissorted_capacity.iloc[:, :n]\n",
    "\n",
    "    b = df_destination_distances_to_cs.values\n",
    "    b.sort(axis=1)\n",
    "    b = pd.DataFrame(b, df_destination_distances_to_cs.index, df_destination_distances_to_cs.columns)\n",
    "    b = b.iloc[:, :n]\n",
    "    d_dissorted_capacity = np.take_along_axis(original_capacity, df_destination_distances_to_cs.values.argsort(axis=1),\n",
    "                                              axis=1)\n",
    "    d_dissorted_capacity = pd.DataFrame(d_dissorted_capacity)\n",
    "    d_dissorted_capacity = d_dissorted_capacity.iloc[:, :n]\n",
    "\n",
    "    train_x = pd.concat([df_od_pairs['duration'], od_dis, a, o_dissorted_capacity, b, d_dissorted_capacity],\n",
    "                        axis=1).iloc[:int(0.7*a.shape[0])]\n",
    "    test_x = pd.concat([df_od_pairs['duration'], od_dis, a, o_dissorted_capacity, b, d_dissorted_capacity],\n",
    "                       axis=1).iloc[int(0.7*a.shape[0]):]\n",
    "    train_y = df_demands['rate'].iloc[:int(0.7*a.shape[0])]\n",
    "    test_y = df_demands['rate'].iloc[int(0.7*a.shape[0]):]\n",
    "#     print(train_x.iloc[:3], train_y.iloc[:3])\n",
    "#     print(test_x.iloc[:3], test_y.iloc[:3])\n",
    "\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    train_x = scaler.fit_transform(train_x)\n",
    "    test_x = scaler.transform(test_x)\n",
    "\n",
    "    gbm = xgboost.XGBRegressor(verbosity=0, n_estimators=100, scale_pos_weight=2,\n",
    "                               validate_parameters=2, learning_rate=0.05,\n",
    "                               min_child_weight=7, max_depth=8) #这行会有个提示，不用管\n",
    "\n",
    "    gbm.fit(train_x, train_y)\n",
    "    predict_y = gbm.predict(test_x)\n",
    "\n",
    "    print('mae:', sklearn.metrics.mean_absolute_error(test_y, predict_y),\n",
    "          'mse:', sklearn.metrics.mean_squared_error(test_y, predict_y))\n",
    "    print('mape:', mape_vectorized_v2(test_y.reshape(1, -1), predict_y))\n",
    "    print('n_rmse:', gbm.score(test_x, test_y))\n",
    "    print(test_y[30:40], predict_y[30:40])\n",
    "\n",
    "print(df_od_pairs.shape[0], 'sample')\n",
    "for i in range(2, 3):\n",
    "    print(i, 'nearest cs as feature:')\n",
    "    evaluate(n=i)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "et",
   "language": "python",
   "display_name": "Python (et)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}