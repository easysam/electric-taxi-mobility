{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils.data_loader as data_loader\n",
    "import utils.display as display\n",
    "import utils.vector_haversine_distances as vec_hs_dis\n",
    "from scipy import stats\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "import sklearn.metrics\n",
    "import sklearn\n",
    "from hotspot.hotpots_discovery_utils import generate_cube_index, cube_to_coordinate\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import make_scorer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "display.configure_logging()\n",
    "display.configure_pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def mape_vectorized_v2(a, b):\n",
    "    mask = a != 0\n",
    "    return (np.fabs(a - b)/a)[mask].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20-Jul-20 23:27:54 - Loading data/transaction_201407.csv\n"
     ]
    }
   ],
   "source": [
    "df_od = data_loader.load_od(scale='full', common=False)\n",
    "# 筛选出处于bbox中的points\n",
    "df_od['in_bbox'] = ((113.764635 < df_od['destination_log'])\n",
    "                     & (df_od['destination_log'] < 114.608972)\n",
    "                     & (22.454727 < df_od['destination_lat'])\n",
    "                     & (df_od['destination_lat'] < 22.842654)\n",
    "                     & (113.764635 < df_od['original_log'])\n",
    "                     & (df_od['original_log'] < 114.608972)\n",
    "                     & (22.454727 < df_od['original_lat'])\n",
    "                     & (df_od['original_lat'] < 22.842654))\n",
    "df_od = df_od.loc[df_od.in_bbox].reset_index(drop=True)\n",
    "df_od = generate_cube_index(df_od, m=100, n=200)\n",
    "\n",
    "demand = df_od.groupby(['original_cube', 'destination_cube']).size().reset_index()\n",
    "demand = demand.rename(columns={0: 'demand'})\n",
    "\n",
    "demand = demand.loc[demand['demand'] > 10].reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20-Jul-20 23:28:18 - Loading data/transaction_common_201407.csv\n"
     ]
    }
   ],
   "source": [
    "df_et_od = data_loader.load_od(scale='full', common=True)\n",
    "# 筛选出处于bbox中的points\n",
    "df_et_od['in_bbox'] = ((113.764635 < df_et_od['destination_log'])\n",
    "                     & (df_et_od['destination_log'] < 114.608972)\n",
    "                     & (22.454727 < df_et_od['destination_lat'])\n",
    "                     & (df_et_od['destination_lat'] < 22.842654)\n",
    "                     & (113.764635 < df_et_od['original_log'])\n",
    "                     & (df_et_od['original_log'] < 114.608972)\n",
    "                     & (22.454727 < df_et_od['original_lat'])\n",
    "                     & (df_et_od['original_lat'] < 22.842654))\n",
    "df_et_od = df_et_od.loc[df_et_od.in_bbox].reset_index(drop=True)\n",
    "df_et_od = generate_cube_index(df_et_od, m=100, n=200)\n",
    "\n",
    "et_demand = df_et_od.groupby(['original_cube', 'destination_cube']).size().reset_index()\n",
    "et_demand = et_demand.rename(columns={0: 'demand'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df_demands = pd.merge(demand, et_demand, how='left', on=['original_cube', 'destination_cube'], suffixes=('_all', '_et'))\n",
    "df_demands = df_demands.fillna(0)\n",
    "df_demands['rate'] = df_demands['demand_et'] / df_demands['demand_all']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "df_od['duration'] = (df_od['end_time'] - df_od['begin_time']).dt.total_seconds()\n",
    "df_od = df_od[['original_cube', 'destination_cube', 'original_log', 'original_lat', 'destination_log',\n",
    "               'destination_lat', 'duration']].groupby(['original_cube', 'destination_cube']).mean()\n",
    "df_od.reset_index(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20-Jul-20 23:28:21 - Loading 'C:\\Users\\hkrep\\PycharmProjects\\ChargingEventsExtraction\\data\\ChargingStation'\n"
     ]
    }
   ],
   "source": [
    "df_od_pairs = pd.merge(df_demands[['original_cube', 'destination_cube']],\n",
    "                       df_od[['original_cube', 'destination_cube', 'original_log', 'original_lat', 'destination_log',\n",
    "                              'destination_lat', 'duration']],\n",
    "                       left_on=['original_cube', 'destination_cube'],\n",
    "                       right_on=['original_cube', 'destination_cube'])\n",
    "\n",
    "# df_od_pairs = df_od_pairs.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_cs, date = data_loader.load_cs(scale='part', date=datetime.datetime(2014, 7, 1))\n",
    "df_cs = df_cs.loc[~df_cs['cs_name'].isin(['LJDL', 'E04', 'BN0002', 'F11', 'S1', 'S2', 'F12', 'F13', 'F15'])]\n",
    "df_cs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# select drop location and CS location as two array\n",
    "original_locations = df_od_pairs[['original_lat', 'original_log']].to_numpy()\n",
    "destination_locations = df_od_pairs[['destination_lat', 'destination_log']].to_numpy()\n",
    "cs_location = df_cs[['Latitude', 'Longitude']].to_numpy()\n",
    "\n",
    "# earth radius(km)\n",
    "AVG_EARTH_RADIUS = 6371.0088\n",
    "\n",
    "# calculate distance between original/destination location and CS location\n",
    "original_distances_to_cs = haversine_distances(np.radians(original_locations), np.radians(cs_location)) \\\n",
    "                           * AVG_EARTH_RADIUS\n",
    "destination_distances_to_cs = haversine_distances(np.radians(destination_locations), np.radians(cs_location)) \\\n",
    "                              * AVG_EARTH_RADIUS\n",
    "# calculate distance between od locations\n",
    "od_dis = vec_hs_dis.haversine_np(df_od_pairs['original_log'], df_od_pairs['original_lat'],\n",
    "                                 df_od_pairs['destination_log'], df_od_pairs['destination_lat'])\n",
    "\n",
    "df_original_distances_to_cs = pd.DataFrame(original_distances_to_cs)\n",
    "df_destination_distances_to_cs = pd.DataFrame(destination_distances_to_cs)\n",
    "df_od_dis = pd.DataFrame(od_dis)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106788 sample\n",
      "2 nearest cs as feature:\n",
      "(85430, 10) (85430,)\n",
      "(21358, 10) (21358,)\n",
      "{'max_depth': 10}\n",
      "-0.043644654226664734\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.05, max_delta_step=0, max_depth=10,\n",
      "             min_child_weight=7, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=2, subsample=1,\n",
      "             tree_method='exact', validate_parameters=2, verbosity=0)\n",
      "2\n",
      "mape: 0.4731577838053306\n",
      "xgb score (rmse in xgb doc): 0.2075826873421407\n",
      "gs score (neg rmse): -0.0436182452604916\n",
      "sample_gt: [0.11764706 0.         0.         0.         0.04929577]\n",
      "sample_pred: [0.05453673 0.04650873 0.05571407 0.0056906  0.04171732]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-c964b331ffb0>:3: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (np.fabs(a - b)/a)[mask].mean()\n"
     ]
    }
   ],
   "source": [
    "def evaluate(n=10):\n",
    "    original_capacity = np.repeat(df_cs['chg_points'].values.reshape(1, -1), df_od_pairs.shape[0], axis=0)\n",
    "\n",
    "    a = df_original_distances_to_cs.values\n",
    "    a.sort(axis=1)\n",
    "    a = pd.DataFrame(a, df_original_distances_to_cs.index, df_original_distances_to_cs.columns)\n",
    "    a = a.iloc[:, :n]\n",
    "    o_dissorted_capacity = np.take_along_axis(original_capacity, df_original_distances_to_cs.values.argsort(axis=1),\n",
    "                                              axis=1)\n",
    "    o_dissorted_capacity = pd.DataFrame(o_dissorted_capacity)\n",
    "    o_dissorted_capacity = o_dissorted_capacity.iloc[:, :n]\n",
    "\n",
    "    b = df_destination_distances_to_cs.values\n",
    "    b.sort(axis=1)\n",
    "    b = pd.DataFrame(b, df_destination_distances_to_cs.index, df_destination_distances_to_cs.columns)\n",
    "    b = b.iloc[:, :n]\n",
    "    d_dissorted_capacity = np.take_along_axis(original_capacity, df_destination_distances_to_cs.values.argsort(axis=1),\n",
    "                                              axis=1)\n",
    "    d_dissorted_capacity = pd.DataFrame(d_dissorted_capacity)\n",
    "    d_dissorted_capacity = d_dissorted_capacity.iloc[:, :n]\n",
    "\n",
    "    train_x = pd.concat([df_od_pairs['duration'], od_dis, a, o_dissorted_capacity, b, d_dissorted_capacity],\n",
    "                        axis=1).iloc[:int(0.7*a.shape[0])]\n",
    "    test_x = pd.concat([df_od_pairs['duration'], od_dis, a, o_dissorted_capacity, b, d_dissorted_capacity],\n",
    "                       axis=1).iloc[int(0.7*a.shape[0]):]\n",
    "    train_y = df_demands['rate'].iloc[:int(0.7*a.shape[0])]\n",
    "    test_y = df_demands['rate'].iloc[int(0.7*a.shape[0]):]\n",
    "#     print(train_x.iloc[:3], train_y.iloc[:3])\n",
    "#     print(test_x.iloc[:3], test_y.iloc[:3])\n",
    "    gbm = xgboost.XGBRegressor(verbosity=0, n_estimators=100, scale_pos_weight=2,\n",
    "                               validate_parameters=2, learning_rate=0.05,\n",
    "                               min_child_weight=7) #这行会有个提示，不用管\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth': [4, 8, 10],\n",
    "        'gamma': [0, 5,],\n",
    "        'max_delta_step': [0, 3],\n",
    "        'subsample': [0.7, 1],\n",
    "        'colsample_bytree': [0.7, 1],\n",
    "        'reg_alpha': [0, 0.1],\n",
    "        'reg_lambda': [1, 3,]\n",
    "    }\n",
    "    # gs = GridSearchCV(gbm, param_grid=param_grid, cv=5, scoring=make_scorer(lambda x, y: 10 - mape_vectorized_v2(x, y)))\n",
    "    gs = GridSearchCV(gbm, param_grid=param_grid, cv=5, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "    train_x, val_x, train_y, val_y = train_test_split(\n",
    "        pd.concat([df_od_pairs['duration'], od_dis, a, o_dissorted_capacity, b, d_dissorted_capacity],axis=1).values,\n",
    "        df_demands['rate'].values, test_size=0.2)\n",
    "    print(train_x.shape, train_y.shape)\n",
    "    print(val_x.shape, val_y.shape)\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    train_x = scaler.fit_transform(train_x)\n",
    "    val_x = scaler.transform(val_x)\n",
    "\n",
    "    gs.fit(train_x, train_y)\n",
    "\n",
    "    print(gs.best_params_)\n",
    "    print(gs.best_score_)\n",
    "    print(gs.best_estimator_)\n",
    "    print(gs.best_index_)\n",
    "\n",
    "    gs.best_estimator_.fit(train_x, train_y)\n",
    "    predict_y = gs.best_estimator_.predict(val_x)\n",
    "\n",
    "    print('mape:', mape_vectorized_v2(val_y.reshape(-1), predict_y))\n",
    "    print('xgb score (rmse in xgb doc):', gs.best_estimator_.score(val_x, val_y))\n",
    "    print('gs score (neg rmse):', gs.score(val_x, val_y))\n",
    "    print('sample_gt:', val_y[10:15])\n",
    "    print('sample_pred:', predict_y[10:15])\n",
    "\n",
    "print(df_od_pairs.shape[0], 'sample')\n",
    "for i in range(1, 10):\n",
    "    print(i, 'nearest cs as feature:')\n",
    "    evaluate(n=i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method RegressorMixin.score of XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n             colsample_bynode=None, colsample_bytree=None, gamma=None,\n             gpu_id=None, importance_type='gain', interaction_constraints=None,\n             learning_rate=0.05, max_delta_step=None, max_depth=None,\n             min_child_weight=7, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             random_state=None, reg_alpha=None, reg_lambda=None,\n             scale_pos_weight=2, subsample=None, tree_method=None,\n             validate_parameters=2, verbosity=0)>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = xgboost.XGBRegressor(verbosity=0, n_estimators=100, scale_pos_weight=2,\n",
    "                               validate_parameters=2, learning_rate=0.05,\n",
    "                               min_child_weight=7)\n",
    "gbm.score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106788 sample\n",
      "2 nearest cs as feature:\n",
      "mae: 0.039303232870928566 mse: 0.002871307803276629\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-17-c4210fe0c932>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'nearest cs as feature:'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 51\u001B[1;33m     \u001B[0mevaluate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-17-c4210fe0c932>\u001B[0m in \u001B[0;36mevaluate\u001B[1;34m(n)\u001B[0m\n\u001B[0;32m     42\u001B[0m     print('mae:', sklearn.metrics.mean_absolute_error(test_y, predict_y),\n\u001B[0;32m     43\u001B[0m           'mse:', sklearn.metrics.mean_squared_error(test_y, predict_y))\n\u001B[1;32m---> 44\u001B[1;33m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'mape:'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmape_vectorized_v2\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_y\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpredict_y\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'n_rmse:'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgbm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mscore\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_x\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_y\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_y\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m30\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m40\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpredict_y\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m30\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m40\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\et\\lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36m__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   5272\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_info_axis\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_can_hold_identifiers_and_holds_name\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5273\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 5274\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mobject\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__getattribute__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   5275\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5276\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__setattr__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "def evaluate(n=10):\n",
    "    original_capacity = np.repeat(df_cs['chg_points'].values.reshape(1, -1), df_od_pairs.shape[0], axis=0)\n",
    "\n",
    "    a = df_original_distances_to_cs.values\n",
    "    a.sort(axis=1)\n",
    "    a = pd.DataFrame(a, df_original_distances_to_cs.index, df_original_distances_to_cs.columns)\n",
    "    a = a.iloc[:, :n]\n",
    "    o_dissorted_capacity = np.take_along_axis(original_capacity, df_original_distances_to_cs.values.argsort(axis=1),\n",
    "                                              axis=1)\n",
    "    o_dissorted_capacity = pd.DataFrame(o_dissorted_capacity)\n",
    "    o_dissorted_capacity = o_dissorted_capacity.iloc[:, :n]\n",
    "\n",
    "    b = df_destination_distances_to_cs.values\n",
    "    b.sort(axis=1)\n",
    "    b = pd.DataFrame(b, df_destination_distances_to_cs.index, df_destination_distances_to_cs.columns)\n",
    "    b = b.iloc[:, :n]\n",
    "    d_dissorted_capacity = np.take_along_axis(original_capacity, df_destination_distances_to_cs.values.argsort(axis=1),\n",
    "                                              axis=1)\n",
    "    d_dissorted_capacity = pd.DataFrame(d_dissorted_capacity)\n",
    "    d_dissorted_capacity = d_dissorted_capacity.iloc[:, :n]\n",
    "\n",
    "    train_x = pd.concat([df_od_pairs['duration'], od_dis, a, o_dissorted_capacity, b, d_dissorted_capacity],\n",
    "                        axis=1).iloc[:int(0.7*a.shape[0])]\n",
    "    test_x = pd.concat([df_od_pairs['duration'], od_dis, a, o_dissorted_capacity, b, d_dissorted_capacity],\n",
    "                       axis=1).iloc[int(0.7*a.shape[0]):]\n",
    "    train_y = df_demands['rate'].iloc[:int(0.7*a.shape[0])]\n",
    "    test_y = df_demands['rate'].iloc[int(0.7*a.shape[0]):]\n",
    "#     print(train_x.iloc[:3], train_y.iloc[:3])\n",
    "#     print(test_x.iloc[:3], test_y.iloc[:3])\n",
    "\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    train_x = scaler.fit_transform(train_x)\n",
    "    test_x = scaler.transform(test_x)\n",
    "\n",
    "    gbm = xgboost.XGBRegressor(verbosity=0, n_estimators=100, scale_pos_weight=2,\n",
    "                               validate_parameters=2, learning_rate=0.05,\n",
    "                               min_child_weight=7, max_depth=8) #这行会有个提示，不用管\n",
    "\n",
    "    gbm.fit(train_x, train_y)\n",
    "    predict_y = gbm.predict(test_x)\n",
    "\n",
    "    print('mae:', sklearn.metrics.mean_absolute_error(test_y, predict_y),\n",
    "          'mse:', sklearn.metrics.mean_squared_error(test_y, predict_y))\n",
    "    print('mape:', mape_vectorized_v2(test_y.reshape(1, -1), predict_y))\n",
    "    print('n_rmse:', gbm.score(test_x, test_y))\n",
    "    print(test_y[30:40], predict_y[30:40])\n",
    "\n",
    "print(df_od_pairs.shape[0], 'sample')\n",
    "for i in range(2, 3):\n",
    "    print(i, 'nearest cs as feature:')\n",
    "    evaluate(n=i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}