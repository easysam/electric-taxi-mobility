{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils.data_loader as data_loader\n",
    "import utils.display as display\n",
    "import utils.vector_haversine_distances as vec_hs_dis\n",
    "from scipy import stats\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "import sklearn.metrics\n",
    "from hotspot.hotpots_discovery_utils import generate_cube_index, cube_to_coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.configure_logging()\n",
    "display.configure_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "def mape_vectorized_v2(a, b):\n",
    "    mask = a != 0\n",
    "    return (np.fabs(a - b)/a)[mask].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17-Jul-20 17:11:16 - Loading data/transaction_201407.csv\n"
     ]
    }
   ],
   "source": [
    "df_od = data_loader.load_od(scale='full', common=False)\n",
    "# 筛选出处于bbox中的points\n",
    "df_od['in_bbox'] = ((113.764635 < df_od['destination_log'])\n",
    "                     & (df_od['destination_log'] < 114.608972)\n",
    "                     & (22.454727 < df_od['destination_lat'])\n",
    "                     & (df_od['destination_lat'] < 22.842654)\n",
    "                     & (113.764635 < df_od['original_log'])\n",
    "                     & (df_od['original_log'] < 114.608972)\n",
    "                     & (22.454727 < df_od['original_lat'])\n",
    "                     & (df_od['original_lat'] < 22.842654))\n",
    "df_od = df_od.loc[df_od.in_bbox].reset_index(drop=True)\n",
    "df_od = generate_cube_index(df_od, m=100, n=200)\n",
    "\n",
    "demand = df_od.groupby(['original_cube', 'destination_cube']).size().reset_index()\n",
    "demand = demand.rename(columns={0: 'demand'})\n",
    "\n",
    "demand = demand.loc[demand['demand'] > 10].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17-Jul-20 17:11:45 - Loading data/transaction_common_201407.csv\n"
     ]
    }
   ],
   "source": [
    "df_et_od = data_loader.load_od(scale='full', common=True)\n",
    "# 筛选出处于bbox中的points\n",
    "df_et_od['in_bbox'] = ((113.764635 < df_et_od['destination_log'])\n",
    "                     & (df_et_od['destination_log'] < 114.608972)\n",
    "                     & (22.454727 < df_et_od['destination_lat'])\n",
    "                     & (df_et_od['destination_lat'] < 22.842654)\n",
    "                     & (113.764635 < df_et_od['original_log'])\n",
    "                     & (df_et_od['original_log'] < 114.608972)\n",
    "                     & (22.454727 < df_et_od['original_lat'])\n",
    "                     & (df_et_od['original_lat'] < 22.842654))\n",
    "df_et_od = df_et_od.loc[df_et_od.in_bbox].reset_index(drop=True)\n",
    "df_et_od = generate_cube_index(df_et_od, m=100, n=200)\n",
    "\n",
    "et_demand = df_et_od.groupby(['original_cube', 'destination_cube']).size().reset_index()\n",
    "et_demand = et_demand.rename(columns={0: 'demand'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demands = pd.merge(demand, et_demand, how='left', on=['original_cube', 'destination_cube'], suffixes=('_all', '_et'))\n",
    "df_demands = df_demands.fillna(0)\n",
    "df_demands['rate'] = df_demands['demand_et'] / df_demands['demand_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df_od['duration'] = (df_od['end_time'] - df_od['begin_time']).dt.total_seconds()\n",
    "df_od = df_od[['original_cube', 'destination_cube', 'original_log', 'original_lat', 'destination_log',\n",
    "               'destination_lat', 'duration']].groupby(['original_cube', 'destination_cube']).mean()\n",
    "df_od.reset_index(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17-Jul-20 17:25:33 - Loading 'C:\\Users\\hkrep\\PycharmProjects\\ChargingEventsExtraction\\data\\ChargingStation'\n"
     ]
    }
   ],
   "source": [
    "df_od_pairs = pd.merge(df_demands[['original_cube', 'destination_cube']],\n",
    "                       df_od[['original_cube', 'destination_cube', 'original_log', 'original_lat', 'destination_log',\n",
    "                              'destination_lat', 'duration']],\n",
    "                       left_on=['original_cube', 'destination_cube'],\n",
    "                       right_on=['original_cube', 'destination_cube'])\n",
    "\n",
    "# df_od_pairs = df_od_pairs.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_cs, date = data_loader.load_cs(scale='part', date=datetime.datetime(2014, 7, 1))\n",
    "df_cs = df_cs.loc[~df_cs['cs_name'].isin(['LJDL', 'E04', 'BN0002', 'F11', 'S1', 'S2', 'F12', 'F13', 'F15'])]\n",
    "df_cs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# select drop location and CS location as two array\n",
    "original_locations = df_od_pairs[['original_lat', 'original_log']].to_numpy()\n",
    "destination_locations = df_od_pairs[['destination_lat', 'destination_log']].to_numpy()\n",
    "cs_location = df_cs[['Latitude', 'Longitude']].to_numpy()\n",
    "\n",
    "# earth radius(km)\n",
    "AVG_EARTH_RADIUS = 6371.0088\n",
    "\n",
    "# calculate distance between original/destination location and CS location\n",
    "original_distances_to_cs = haversine_distances(np.radians(original_locations), np.radians(cs_location)) \\\n",
    "                           * AVG_EARTH_RADIUS\n",
    "destination_distances_to_cs = haversine_distances(np.radians(destination_locations), np.radians(cs_location)) \\\n",
    "                              * AVG_EARTH_RADIUS\n",
    "# calculate distance between od locations\n",
    "od_dis = vec_hs_dis.haversine_np(df_od_pairs['original_log'], df_od_pairs['original_lat'],\n",
    "                                 df_od_pairs['destination_log'], df_od_pairs['destination_lat'])\n",
    "\n",
    "df_original_distances_to_cs = pd.DataFrame(original_distances_to_cs)\n",
    "df_destination_distances_to_cs = pd.DataFrame(destination_distances_to_cs)\n",
    "df_od_dis = pd.DataFrame(od_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106788 sample\n",
      "5 nearest cs as feature:\n",
      "{'colsample_bytree': 1, 'gamma': 0, 'max_delta_step': 0, 'max_depth': 8, 'reg_alpha': 0, 'reg_lambda': 1, 'subsample': 1}\n",
      "9.539238289611852\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.05, max_delta_step=0, max_depth=8,\n",
      "             min_child_weight=7, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=2, subsample=1,\n",
      "             tree_method='exact', validate_parameters=2, verbosity=0)\n",
      "73\n",
      "mape: 0.4590369754097927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hkrep\\anaconda3\\envs\\et\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "def evaluate(n=10):\n",
    "    original_capacity = np.repeat(df_cs['chg_points'].values.reshape(1, -1), df_od_pairs.shape[0], axis=0)\n",
    "    \n",
    "    a = df_original_distances_to_cs.values\n",
    "    a.sort(axis=1)\n",
    "    a = pd.DataFrame(a, df_original_distances_to_cs.index, df_original_distances_to_cs.columns)\n",
    "    a = a.iloc[:, :n]\n",
    "    o_dissorted_capacity = np.take_along_axis(original_capacity, df_original_distances_to_cs.values.argsort(axis=1),\n",
    "                                              axis=1)\n",
    "    o_dissorted_capacity = pd.DataFrame(o_dissorted_capacity)\n",
    "    o_dissorted_capacity = o_dissorted_capacity.iloc[:, :n]\n",
    "    \n",
    "    b = df_destination_distances_to_cs.values\n",
    "    b.sort(axis=1)\n",
    "    b = pd.DataFrame(b, df_destination_distances_to_cs.index, df_destination_distances_to_cs.columns)\n",
    "    b = b.iloc[:, :n]\n",
    "    d_dissorted_capacity = np.take_along_axis(original_capacity, df_destination_distances_to_cs.values.argsort(axis=1),\n",
    "                                              axis=1)\n",
    "    d_dissorted_capacity = pd.DataFrame(d_dissorted_capacity)\n",
    "    d_dissorted_capacity = d_dissorted_capacity.iloc[:, :n]\n",
    "    \n",
    "    train_x = pd.concat([df_od_pairs['duration'], od_dis, a, o_dissorted_capacity, b, d_dissorted_capacity],\n",
    "                        axis=1).iloc[:int(0.7*a.shape[0])]\n",
    "    test_x = pd.concat([df_od_pairs['duration'], od_dis, a, o_dissorted_capacity, b, d_dissorted_capacity],\n",
    "                       axis=1).iloc[int(0.7*a.shape[0]):]\n",
    "    train_y = df_demands['rate'].iloc[:int(0.7*a.shape[0])]\n",
    "    test_y = df_demands['rate'].iloc[int(0.7*a.shape[0]):]\n",
    "#     print(train_x.iloc[:3], train_y.iloc[:3])\n",
    "#     print(test_x.iloc[:3], test_y.iloc[:3])\n",
    "    gbm = xgboost.XGBRegressor(verbosity=0, n_estimators=100, scale_pos_weight=2,\n",
    "                               validate_parameters=2, learning_rate=0.05,\n",
    "                               min_child_weight=7) #这行会有个提示，不用管\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth': [4, 8],\n",
    "        'gamma': [0, 5,],\n",
    "        'max_delta_step': [0, 3],\n",
    "        'subsample': [0.7, 1],\n",
    "        'colsample_bytree': [0.7, 1],\n",
    "        'reg_alpha': [0, 0.1],\n",
    "        'reg_lambda': [1, 3,]\n",
    "    }\n",
    "    gs = GridSearchCV(gbm, param_grid=param_grid, cv=5, scoring=make_scorer(lambda x, y: 10 - mape_vectorized_v2(x, y)))\n",
    "\n",
    "    train_x, val_x, train_y, val_y = train_test_split(\n",
    "        pd.concat([df_od_pairs['duration'], od_dis, a, o_dissorted_capacity, b, d_dissorted_capacity],axis=1).values,\n",
    "        df_demands['rate'].values, test_size=0.2)\n",
    "\n",
    "    gs.fit(train_x, train_y)\n",
    "\n",
    "    print(gs.best_params_)\n",
    "    print(gs.best_score_)\n",
    "    print(gs.best_estimator_)\n",
    "    print(gs.best_index_)\n",
    "\n",
    "    gs.best_estimator_.fit(train_x, train_y)\n",
    "    predict_y = gs.best_estimator_.predict(val_x)\n",
    "    \n",
    "    # print(sklearn.metrics.mean_absolute_error(val_y, predict_y), sklearn.metrics.mean_squared_error(val_y, predict_y))\n",
    "    print('mape:', mape_vectorized_v2(val_y, predict_y))\n",
    "\n",
    "print(df_od_pairs.shape[0], 'sample')\n",
    "for i in range(5, 6):\n",
    "    print(i, 'nearest cs as feature:')\n",
    "    evaluate(n=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import make_scorer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106788 sample\n",
      "5 nearest cs as feature:\n",
      "0.04432376258435436 0.003580318491623788\n",
      "mape: 0.6076186987027307\n"
     ]
    }
   ],
   "source": [
    "def evaluate(n=10):\n",
    "    original_capacity = np.repeat(df_cs['chg_points'].values.reshape(1, -1), df_od_pairs.shape[0], axis=0)\n",
    "\n",
    "    a = df_original_distances_to_cs.values\n",
    "    a.sort(axis=1)\n",
    "    a = pd.DataFrame(a, df_original_distances_to_cs.index, df_original_distances_to_cs.columns)\n",
    "    a = a.iloc[:, :n]\n",
    "    o_dissorted_capacity = np.take_along_axis(original_capacity, df_original_distances_to_cs.values.argsort(axis=1),\n",
    "                                              axis=1)\n",
    "    o_dissorted_capacity = pd.DataFrame(o_dissorted_capacity)\n",
    "    o_dissorted_capacity = o_dissorted_capacity.iloc[:, :n]\n",
    "\n",
    "    b = df_destination_distances_to_cs.values\n",
    "    b.sort(axis=1)\n",
    "    b = pd.DataFrame(b, df_destination_distances_to_cs.index, df_destination_distances_to_cs.columns)\n",
    "    b = b.iloc[:, :n]\n",
    "    d_dissorted_capacity = np.take_along_axis(original_capacity, df_destination_distances_to_cs.values.argsort(axis=1),\n",
    "                                              axis=1)\n",
    "    d_dissorted_capacity = pd.DataFrame(d_dissorted_capacity)\n",
    "    d_dissorted_capacity = d_dissorted_capacity.iloc[:, :n]\n",
    "\n",
    "    train_x = pd.concat([df_od_pairs['duration'], od_dis, a, o_dissorted_capacity, b, d_dissorted_capacity],\n",
    "                        axis=1).iloc[:int(0.7*a.shape[0])]\n",
    "    test_x = pd.concat([df_od_pairs['duration'], od_dis, a, o_dissorted_capacity, b, d_dissorted_capacity],\n",
    "                       axis=1).iloc[int(0.7*a.shape[0]):]\n",
    "    train_y = df_demands['rate'].iloc[:int(0.7*a.shape[0])]\n",
    "    test_y = df_demands['rate'].iloc[int(0.7*a.shape[0]):]\n",
    "#     print(train_x.iloc[:3], train_y.iloc[:3])\n",
    "#     print(test_x.iloc[:3], test_y.iloc[:3])\n",
    "    gbm = xgboost.XGBRegressor(verbosity=1, max_depth=10, n_estimators=100, scale_pos_weight=2) #这行会有个提示，不用管\n",
    "\n",
    "    gbm.fit(train_x.values, train_y.values)\n",
    "    predict_y = gbm.predict(test_x.values)\n",
    "\n",
    "    print(sklearn.metrics.mean_absolute_error(test_y, predict_y), sklearn.metrics.mean_squared_error(test_y, predict_y))\n",
    "    print('mape:', mape_vectorized_v2(test_y, predict_y))\n",
    "\n",
    "print(df_od_pairs.shape[0], 'sample')\n",
    "for i in range(5, 6):\n",
    "    print(i, 'nearest cs as feature:')\n",
    "    evaluate(n=i)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "et",
   "language": "python",
   "display_name": "Python (et)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}