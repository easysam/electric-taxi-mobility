{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils.data_loader as data_loader\n",
    "import utils.display as display\n",
    "from scipy import stats\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "import sklearn.metrics\n",
    "from hotspot.hotpots_discovery_utils import generate_cube_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.configure_logging()\n",
    "display.configure_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14-Jul-20 20:02:31 - Loading C:\\Users\\hkrept\\PycharmProjects\\ElectricVehicleMobility\\data\\transaction_201407.csv\n"
     ]
    }
   ],
   "source": [
    "df_od = data_loader.load_od(scale='full', common=False)\n",
    "# 筛选出处于bbox中的points\n",
    "df_od['in_bbox'] = ((113.764635 < df_od['destination_log'])\n",
    "                     & (df_od['destination_log'] < 114.608972)\n",
    "                     & (22.454727 < df_od['destination_lat'])\n",
    "                     & (df_od['destination_lat'] < 22.842654)\n",
    "                     & (113.764635 < df_od['original_log'])\n",
    "                     & (df_od['original_log'] < 114.608972)\n",
    "                     & (22.454727 < df_od['original_lat'])\n",
    "                     & (df_od['original_lat'] < 22.842654))\n",
    "df_od = df_od.loc[df_od.in_bbox].reset_index(drop=True)\n",
    "df_od = generate_cube_index(df_od, m=100, n=200)\n",
    "\n",
    "demand = df_od.groupby(['original_cube', 'destination_cube']).size().reset_index()\n",
    "demand = demand.rename(columns={0: 'demand'})\n",
    "\n",
    "demand = demand.loc[demand['demand'] > 10].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14-Jul-20 20:03:01 - Loading C:\\Users\\hkrept\\PycharmProjects\\ElectricVehicleMobility\\data/transaction_common_201407.csv\n"
     ]
    }
   ],
   "source": [
    "df_et_od = data_loader.load_od(scale='full', common=True)\n",
    "# 筛选出处于bbox中的points\n",
    "df_et_od['in_bbox'] = ((113.764635 < df_et_od['destination_log'])\n",
    "                     & (df_et_od['destination_log'] < 114.608972)\n",
    "                     & (22.454727 < df_et_od['destination_lat'])\n",
    "                     & (df_et_od['destination_lat'] < 22.842654)\n",
    "                     & (113.764635 < df_et_od['original_log'])\n",
    "                     & (df_et_od['original_log'] < 114.608972)\n",
    "                     & (22.454727 < df_et_od['original_lat'])\n",
    "                     & (df_et_od['original_lat'] < 22.842654))\n",
    "df_et_od = df_et_od.loc[df_et_od.in_bbox].reset_index(drop=True)\n",
    "df_et_od = generate_cube_index(df_et_od, m=100, n=200)\n",
    "\n",
    "et_demand = df_et_od.groupby(['original_cube', 'destination_cube']).size().reset_index()\n",
    "et_demand = et_demand.rename(columns={0: 'demand'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demands = pd.merge(demand, et_demand, how='left', on=['original_cube', 'destination_cube'], suffixes=('_all', '_et'))\n",
    "df_demands = df_demands.fillna(0)\n",
    "df_demands['rate'] = df_demands['demand_et'] / df_demands['demand_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14-Jul-20 20:03:03 - Loading 'C:\\Users\\hkrep\\PycharmProjects\\ChargingEventsExtraction\\data\\ChargingStation'\n"
     ]
    }
   ],
   "source": [
    "temp = df_od.drop_duplicates(['original_cube', 'destination_cube'])\n",
    "df_od_pairs = pd.merge(df_demands[['original_cube', 'destination_cube']],\n",
    "                       df_od[['original_cube', 'destination_cube', 'original_log', 'original_lat', \n",
    "                              'destination_log', 'destination_lat']].drop_duplicates(['original_cube', 'destination_cube']),\n",
    "                       left_on=['original_cube', 'destination_cube'], \n",
    "                       right_on=['original_cube', 'destination_cube'])\n",
    "\n",
    "df_cs, date = data_loader.load_cs(scale='part', date=datetime.datetime(2014, 7, 1))\n",
    "df_cs = df_cs.loc[~df_cs['cs_name'].isin(['LJDL', 'E04', 'BN0002', 'F11', 'S1', 'S2', 'F12', 'F13', 'F15'])]\n",
    "df_cs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# select drop location and CS location as two array\n",
    "original_locations = df_od_pairs[['original_lat', 'original_log']].to_numpy()\n",
    "destination_locations = df_od_pairs[['destination_lat', 'destination_log']].to_numpy()\n",
    "cs_location = df_cs[['Latitude', 'Longitude']].to_numpy()\n",
    "\n",
    "# earth radius(km)\n",
    "AVG_EARTH_RADIUS = 6371.0088\n",
    "\n",
    "# calculate distance between drop location and CS location, and midian, min, max, mean\n",
    "original_distances_to_cs = haversine_distances(np.radians(original_locations), np.radians(cs_location)) * AVG_EARTH_RADIUS\n",
    "destination_distances_to_cs = haversine_distances(np.radians(destination_locations), np.radians(cs_location)) * AVG_EARTH_RADIUS\n",
    "df_original_distances_to_cs = pd.DataFrame(original_distances_to_cs)\n",
    "df_destination_distances_to_cs = pd.DataFrame(destination_distances_to_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_vectorized_v2(a, b): \n",
    "    mask = a != 0\n",
    "    return (np.fabs(a - b)/a)[mask].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106788 sample\n",
      "1 nearest cs as feature:\n",
      "0.03986787878145368 0.0030707246156373227\n",
      "0.5130642375794054\n",
      "2 nearest cs as feature:\n",
      "0.03951798126982014 0.002931755808633905\n",
      "0.496760343910151\n",
      "3 nearest cs as feature:\n",
      "0.0404078168934603 0.0028932540690789856\n",
      "0.5159218848383474\n",
      "4 nearest cs as feature:\n",
      "0.04072224289728923 0.0029544486074978117\n",
      "0.5238698955755167\n",
      "5 nearest cs as feature:\n",
      "0.045270584206721594 0.0034302777112109758\n",
      "0.6179571665356273\n",
      "6 nearest cs as feature:\n",
      "0.04390052259172482 0.0032466943156795693\n",
      "0.5858621264542101\n",
      "7 nearest cs as feature:\n",
      "0.043715177914717455 0.0032192182829961775\n",
      "0.587664423245331\n",
      "8 nearest cs as feature:\n",
      "0.043553875081353784 0.003155401089737722\n",
      "0.5846863814080752\n",
      "9 nearest cs as feature:\n",
      "0.04287185209869326 0.0031119952320785844\n",
      "0.5680124509405862\n",
      "10 nearest cs as feature:\n",
      "0.04465659364153703 0.0032990106654289257\n",
      "0.6126721846546823\n",
      "11 nearest cs as feature:\n",
      "0.043736652105987636 0.0031931422753202375\n",
      "0.5924421813005173\n",
      "12 nearest cs as feature:\n",
      "0.044420702023774024 0.003314015558186356\n",
      "0.5992528137855734\n",
      "13 nearest cs as feature:\n",
      "0.045665011376699295 0.0034704194700313803\n",
      "0.6229333728549016\n",
      "14 nearest cs as feature:\n",
      "0.04518155753305706 0.003413345732079942\n",
      "0.6102527907638174\n",
      "15 nearest cs as feature:\n",
      "0.044208540574217706 0.0032867907960032324\n",
      "0.5924364984503777\n",
      "16 nearest cs as feature:\n",
      "0.04483406473756038 0.003340467082633455\n",
      "0.6082345720488711\n",
      "17 nearest cs as feature:\n",
      "0.044560424508224616 0.0033142556466680953\n",
      "0.6051483987620183\n",
      "18 nearest cs as feature:\n",
      "0.04383191024299511 0.003212408461416617\n",
      "0.5906940122994457\n",
      "19 nearest cs as feature:\n",
      "0.04380978549145617 0.0032061764986699845\n",
      "0.5842492105605127\n",
      "20 nearest cs as feature:\n",
      "0.04391477140363287 0.003233733998404896\n",
      "0.5896587263332524\n",
      "21 nearest cs as feature:\n",
      "0.043491503729763764 0.0031713824098416623\n",
      "0.5808187539283483\n",
      "22 nearest cs as feature:\n",
      "0.042694539762253146 0.0030821820511353713\n",
      "0.5582478738119024\n",
      "23 nearest cs as feature:\n",
      "0.044614083023474964 0.00330165314713669\n",
      "0.5997676777121348\n"
     ]
    }
   ],
   "source": [
    "def evaluate(n=10):\n",
    "    original_capacity = np.repeat(df_cs['chg_points'].values.reshape(1, -1), df_od_pairs.shape[0], axis=0)\n",
    "    \n",
    "    a = df_original_distances_to_cs.values\n",
    "    a.sort(axis=1)\n",
    "    a = pd.DataFrame(a, df_original_distances_to_cs.index, df_original_distances_to_cs.columns)\n",
    "    a = a.iloc[:, :n]\n",
    "    o_dissorted_capacity = np.take_along_axis(original_capacity, df_original_distances_to_cs.values.argsort(axis=1), axis=1)\n",
    "    o_dissorted_capacity = pd.DataFrame(o_dissorted_capacity)\n",
    "    o_dissorted_capacity = o_dissorted_capacity.iloc[:, :n]\n",
    "    \n",
    "    b = df_destination_distances_to_cs.values\n",
    "    b.sort(axis=1)\n",
    "    b = pd.DataFrame(b, df_destination_distances_to_cs.index, df_destination_distances_to_cs.columns)\n",
    "    b = b.iloc[:, :n]\n",
    "    d_dissorted_capacity = np.take_along_axis(original_capacity, df_destination_distances_to_cs.values.argsort(axis=1), axis=1)\n",
    "    d_dissorted_capacity = pd.DataFrame(d_dissorted_capacity)\n",
    "    d_dissorted_capacity = d_dissorted_capacity.iloc[:, :n]\n",
    "    \n",
    "    train_x = pd.concat([a, o_dissorted_capacity, b, d_dissorted_capacity], axis=1).iloc[:int(0.7*a.shape[0])]\n",
    "    test_x = pd.concat([a, o_dissorted_capacity, b, d_dissorted_capacity], axis=1).iloc[int(0.7*a.shape[0]):]\n",
    "    train_y = df_demands['rate'].iloc[:int(0.7*a.shape[0])]\n",
    "    test_y = df_demands['rate'].iloc[int(0.7*a.shape[0]):]\n",
    "#     print(train_x.iloc[:3], train_y.iloc[:3])\n",
    "#     print(test_x.iloc[:3], test_y.iloc[:3])\n",
    "    gbm = xgboost.XGBRegressor(verbosity=1, max_depth=10, learning_rate=0.05, n_estimators=500, scale_pos_weight=2) #这行会有个提示，不用管\n",
    "    gbm.fit(train_x.values, train_y.values)\n",
    "    predict_y = gbm.predict(test_x.values)\n",
    "    \n",
    "    print(sklearn.metrics.mean_absolute_error(test_y, predict_y), sklearn.metrics.mean_squared_error(test_y, predict_y))\n",
    "    print(mape_vectorized_v2(test_y, predict_y))\n",
    "\n",
    "print(df_od_pairs.shape[0], 'sample')\n",
    "for i in range(1, 24):\n",
    "    print(i, 'nearest cs as feature:')\n",
    "    evaluate(n=i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
